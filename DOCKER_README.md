# ğŸ³ Executando EstatiCar com Docker/Podman

Este guia mostra como executar a aplicaÃ§Ã£o EstatiCar usando containers Docker ou Podman.

## ğŸ“‹ PrÃ©-requisitos

- Docker **ou** Podman
- Docker Compose **ou** Podman Compose

> **Nota**: Os scripts de setup detectam automaticamente se vocÃª estÃ¡ usando Docker ou Podman.

## ğŸš€ InÃ­cio RÃ¡pido

### 1. Iniciar os serviÃ§os

**OpÃ§Ã£o A - Script AutomÃ¡tico (Recomendado)**:

```bash
# Windows (PowerShell)
.\docker-setup.ps1

# Linux/Mac
chmod +x docker-setup.sh
./docker-setup.sh
```

**OpÃ§Ã£o B - Manual**:

No diretÃ³rio `app/`, execute (substituindo `docker` por `podman` se necessÃ¡rio):

```bash
# Criar diretÃ³rios necessÃ¡rios primeiro
mkdir -p uploads build_assets

# Subir os containers
docker compose up -d
# ou
podman compose up -d
```

Isso iniciarÃ¡ trÃªs serviÃ§os em uma rede isolada (`estaticar-network`):
- **estaticar**: A aplicaÃ§Ã£o principal (portas 8080 e 8081)
- **qdrant**: Banco de dados vetorial (porta 6333)
- **ollama**: Servidor LLM (porta 11434)

Os serviÃ§os usam **health checks** para garantir que Qdrant e Ollama estejam prontos antes de iniciar a aplicaÃ§Ã£o.

### 2. Baixar um modelo Ollama

Antes de usar a aplicaÃ§Ã£o, vocÃª precisa baixar um modelo LLM no container Ollama:

```bash
docker exec -it ollama ollama pull mistral
# ou
podman exec -it ollama ollama pull mistral
```

Ou outro modelo de sua preferÃªncia (ex: `llama2`, `llama3`, `codellama`, `phi`, `qwen`).

> **Dica**: Use `docker exec -it ollama ollama list` para ver os modelos disponÃ­veis localmente.

### 3. Acessar a aplicaÃ§Ã£o

Abra seu navegador em:
- **Interface Web**: http://localhost:8080

## ğŸ”§ Comandos Ãšteis

> **Nota**: Nos exemplos abaixo, substitua `docker` por `podman` se estiver usando Podman.

### Ver logs

```bash
# Todos os serviÃ§os
docker compose logs -f

# Apenas a aplicaÃ§Ã£o
docker compose logs -f estaticar

# Apenas o Ollama
docker compose logs -f ollama
```

### Parar os serviÃ§os

```bash
docker compose down
```

### Parar e remover volumes (dados)

```bash
docker compose down -v
```

### Reconstruir a imagem

```bash
docker compose build --no-cache
docker compose up -d
```

## ğŸ® Gerenciar modelos Ollama

### Listar modelos instalados

```bash
docker exec -it ollama ollama list
```

### Baixar um modelo

```bash
docker exec -it ollama ollama pull <nome-do-modelo>
```

### Remover um modelo

```bash
docker exec -it ollama ollama rm <nome-do-modelo>
```

### Testar um modelo

```bash
docker exec -it ollama ollama run mistral "Hello, how are you?"
```

## ğŸ“‚ Volumes e PersistÃªncia

Os seguintes dados sÃ£o persistidos em volumes Docker:

- **qdrant_storage**: Dados do banco vetorial (embeddings, treinamento)
- **ollama_data**: Modelos LLM baixados

Os seguintes dados sÃ£o mapeados para o host:

- **app/build_assets**: ConfiguraÃ§Ãµes e dados da aplicaÃ§Ã£o
- **app/uploads**: Arquivos enviados via interface

## ğŸ–¥ï¸ GPU Support (Opcional)

Se vocÃª tem uma GPU NVIDIA, pode habilitar aceleraÃ§Ã£o GPU para o Ollama:

1. Instale o [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html)

2. No arquivo `compose.yml`, descomente as linhas da seÃ§Ã£o `deploy` do serviÃ§o `ollama`:

```yaml
ollama:
  # ...
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]
```

3. Reinicie os containers:

```bash
docker compose down
docker compose up -d
```

## ğŸ” Troubleshooting

### Erro "no such file or directory" ao montar volumes

Se vocÃª ver um erro como `statfs /mnt/c/Users/.../uploads: no such file or directory`:

```bash
# Criar os diretÃ³rios necessÃ¡rios
cd app
mkdir -p uploads build_assets

# Tentar novamente
podman compose up -d
```

Os scripts automÃ¡ticos (`docker-setup.ps1` / `docker-setup.sh`) jÃ¡ criam esses diretÃ³rios automaticamente.

### Erro "No module named flet.__main__"

Este erro ocorre quando hÃ¡ conflito entre o cÃ³digo no container e os volume mounts. Para corrigir:

```bash
# Parar e remover o container
podman stop estaticar
podman rm estaticar

# Reconstruir a imagem
cd app
podman compose build --no-cache

# Iniciar novamente
podman compose up -d
```

**Nota importante**: O arquivo `compose.yml` nÃ£o monta o diretÃ³rio de cÃ³digo fonte (`../app`) por padrÃ£o para evitar conflitos com as dependÃªncias instaladas no container. Se vocÃª precisa fazer desenvolvimento com hot-reload, veja a seÃ§Ã£o "Modo Desenvolvimento" abaixo.

### A aplicaÃ§Ã£o nÃ£o conecta ao Qdrant

Verifique se o serviÃ§o estÃ¡ rodando:
```bash
docker compose ps
curl http://localhost:6333/collections
```

### A aplicaÃ§Ã£o nÃ£o encontra o modelo Ollama

1. Verifique se o modelo estÃ¡ instalado:
   ```bash
   docker exec -it ollama ollama list
   ```

2. Baixe o modelo necessÃ¡rio:
   ```bash
   docker exec -it ollama ollama pull mistral
   ```

### Erro de porta jÃ¡ em uso

Se as portas 8080, 8081, 6333 ou 11434 jÃ¡ estiverem em uso, vocÃª pode alterÃ¡-las no `compose.yml`:

```yaml
services:
  estaticar:
    ports:
      - "9080:8080"  # Altere 9080 para a porta desejada
      - "9081:8081"
```

### Verificar conectividade entre containers

Para testar se os containers conseguem se comunicar na rede:

```bash
# Verificar se Qdrant estÃ¡ acessÃ­vel
docker exec -it estaticar wget -q -O- http://qdrant:6333/health

# Verificar se Ollama estÃ¡ acessÃ­vel
docker exec -it estaticar nc -zv ollama 11434

# Ver todas as redes
docker network ls

# Inspecionar a rede estaticar
docker network inspect app_estaticar-network
```

## ğŸ“ VariÃ¡veis de Ambiente

As seguintes variÃ¡veis podem ser configuradas no `compose.yml`:

- `QDRANT_URL`: URL do serviÃ§o Qdrant (padrÃ£o: `http://qdrant:6333`)
- `OLLAMA_HOST`: URL do serviÃ§o Ollama (padrÃ£o: `http://ollama:11434`)
- `PYTHONUNBUFFERED`: Modo de logging Python (padrÃ£o: `1`)

## ğŸ› ï¸ Modo Desenvolvimento

Por padrÃ£o, o cÃ³digo da aplicaÃ§Ã£o fica dentro do container para evitar conflitos com dependÃªncias. Se vocÃª precisa fazer alteraÃ§Ãµes no cÃ³digo com hot-reload:

1. Edite o arquivo `compose.yml` e descomente a linha de volume mount:

```yaml
volumes:
  - ../app:/app/app  # Descomente esta linha
  - ./build_assets:/app/app/build_assets
  - ./uploads:/app/app/uploads
```

2. Reconstrua e reinicie:

```bash
podman compose down
podman compose build --no-cache
podman compose up -d
```

**Aviso**: Montar o diretÃ³rio do cÃ³digo pode sobrescrever as dependÃªncias instaladas. Use apenas durante desenvolvimento.

## ğŸ—ï¸ Arquitetura

Os containers estÃ£o conectados em uma rede bridge isolada (`estaticar-network`):

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         estaticar-network (bridge)                  â”‚
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚  â”‚  Navegador  â”‚ (host)                            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚         â”‚ :8080 (web) / :8081 (downloads)          â”‚
â”‚         â”‚                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚  â”‚  EstatiCar  â”‚                                   â”‚
â”‚  â”‚  Container  â”‚                                   â”‚
â”‚  â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”˜                                   â”‚
â”‚     â”‚        â”‚                                      â”‚
â”‚     â”‚:6333   â”‚:11434                               â”‚
â”‚     â”‚        â”‚                                      â”‚
â”‚  â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”‚                                     â”‚
â”‚  â”‚ Qdrant  â”‚ â”‚  Health checks:                     â”‚
â”‚  â”‚(Vector) â”‚ â”‚  - Qdrant: wget health endpoint     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  - Ollama: ollama list              â”‚
â”‚              â”‚                                      â”‚
â”‚        â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”                                â”‚
â”‚        â”‚  Ollama  â”‚                                â”‚
â”‚        â”‚   (LLM)  â”‚                                â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Volumes Persistentes:
  - qdrant_storage  â†’ Embeddings e dados vetoriais
  - ollama_data     â†’ Modelos LLM baixados
```

### BenefÃ­cios da Rede Isolada

- **Isolamento**: Os containers se comunicam apenas entre si
- **DNS interno**: Resolvem-se por nome (qdrant, ollama, estaticar)
- **SeguranÃ§a**: Sem exposiÃ§Ã£o desnecessÃ¡ria ao host
- **Health checks**: AplicaÃ§Ã£o sÃ³ inicia quando dependÃªncias estÃ£o prontas

## ğŸ“œ LicenÃ§a

Ver arquivo LICENSE no repositÃ³rio principal.
